{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42a1c332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "import re\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "188a37c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>retweet</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-18 23:17:14 SE Asia Standard Time</td>\n",
       "      <td>1.170000e+18</td>\n",
       "      <td>njmyg_</td>\n",
       "      <td>@sunnova1324 @tanyainrl iya sih.. tapi maksud ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-18 23:07:06 SE Asia Standard Time</td>\n",
       "      <td>1.310000e+18</td>\n",
       "      <td>urbbyyyyyyyy</td>\n",
       "      <td>males kuliah online temennya sikit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-11-18 23:05:21 SE Asia Standard Time</td>\n",
       "      <td>1.330000e+18</td>\n",
       "      <td>risyaanggun</td>\n",
       "      <td>tumbenan td kuliah online dosennya minta join ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-11-18 22:58:58 SE Asia Standard Time</td>\n",
       "      <td>1.240000e+18</td>\n",
       "      <td>nyctophilexxx</td>\n",
       "      <td>@monsouleil nangis krn kecapean kuliah online ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-11-18 22:50:53 SE Asia Standard Time</td>\n",
       "      <td>1.090000e+18</td>\n",
       "      <td>anisanwl</td>\n",
       "      <td>Apa hanya aku yang merasa semenjak kuliah onli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  created_at       user_id       username  \\\n",
       "0  2020-11-18 23:17:14 SE Asia Standard Time  1.170000e+18         njmyg_   \n",
       "1  2020-11-18 23:07:06 SE Asia Standard Time  1.310000e+18   urbbyyyyyyyy   \n",
       "2  2020-11-18 23:05:21 SE Asia Standard Time  1.330000e+18    risyaanggun   \n",
       "3  2020-11-18 22:58:58 SE Asia Standard Time  1.240000e+18  nyctophilexxx   \n",
       "4  2020-11-18 22:50:53 SE Asia Standard Time  1.090000e+18       anisanwl   \n",
       "\n",
       "                                               tweet  replies_count  \\\n",
       "0  @sunnova1324 @tanyainrl iya sih.. tapi maksud ...              0   \n",
       "1                 males kuliah online temennya sikit              0   \n",
       "2  tumbenan td kuliah online dosennya minta join ...              0   \n",
       "3  @monsouleil nangis krn kecapean kuliah online ...              1   \n",
       "4  Apa hanya aku yang merasa semenjak kuliah onli...              0   \n",
       "\n",
       "   retweets_count  likes_count  retweet  polarity  \n",
       "0               0            0    False  positive  \n",
       "1               0            0    False  negative  \n",
       "2               0            0    False  negative  \n",
       "3               0            0    False  negative  \n",
       "4               0            1    False  negative  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load Dataset\n",
    "# data_frame = pd.read_csv('tweets_data.csv')\n",
    "data_frame = pd.read_csv('Dataset/tweets_data.csv')\n",
    "# data_frame = data_frame[:5000]\n",
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "994d38a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b01754",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd7c7d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    12733\n",
       "1.0     8441\n",
       "0.0     2525\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame['polarity'].replace(('neutral', 'positive', 'negative'), (0, 1, 2), inplace=True)\n",
    "data_frame['polarity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74cccaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_frame['tweet'].values.tolist()\n",
    "label = data_frame['polarity'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05fe27d",
   "metadata": {},
   "source": [
    "# Split Data train dan Data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96911694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training: 20000\n",
      "Jumlah data testing: 5000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, y_train, y_test = train_test_split(data, label, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(f'Jumlah data training: {len(train_X)}')\n",
    "print(f'Jumlah data testing: {len(test_X)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb8bcf",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f848b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text_clean = re.sub(r'@[A-Za-z0-9]+', '', str(text)) # Hapus Mention\n",
    "    text_clean = re.sub(r'#[A-Za-z0-9]+', '', text_clean) # Hapus hashtag\n",
    "    text_clean = re.sub(r'RT[\\s]', '', text_clean) # Hapus RT\n",
    "    text_clean = re.sub(r\"http\\S+\", '', text_clean) # Hapus link\n",
    "    text_clean = re.sub(r'[0-9]+', '', text_clean) # Hapus angka\n",
    "    text_clean = text_clean.replace('\\n', ' ') # Ganti enter ke spasi\n",
    "    text_clean = text_clean.translate(str.maketrans('', '', string.punctuation)) # Hapus tanda baca\n",
    "    text_clean = text_clean.strip(' ') # Hapus spasi tdk jelas\n",
    "    return text_clean\n",
    "\n",
    "def casefoldingText(text):\n",
    "    lwr = text\n",
    "    map(str.lower, lwr)\n",
    "    text = lwr\n",
    "    return text\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "def tokenizingText(text): \n",
    "    text = tokenizer.tokenize(text)                  \n",
    "    return text\n",
    "\n",
    "def filteringText(text):\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered \n",
    "    return text\n",
    "\n",
    "def stemmingText(text): \n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    text = [stemmer.stem(word) for word in text]\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7290d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet_Cleaning = cleaningText(tweet)\n",
    "    tweet_CaseFolding = casefoldingText(tweet_Cleaning)\n",
    "    tweet_Tokenizing = tokenizingText(tweet_CaseFolding)\n",
    "    tweet_Filtering = filteringText(tweet_Tokenizing)\n",
    "#     tweet_Stemming = stemmingText(tweet_Filtering)    \n",
    "    return tweet_Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d98409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mulai: 20:52:22\n",
      "Selesai: 20:52:34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "time_jkt = pytz.timezone('Asia/Jakarta')\n",
    "print(\"Mulai:\", datetime.now(time_jkt).strftime(\"%H:%M:%S\"))\n",
    "\n",
    "#preprocess data train\n",
    "preprocessed_text = []\n",
    "for i in range(0, len(train_X)):\n",
    "    preprocessed_text.append(process_tweet(train_X[i]))\n",
    "\n",
    "X_train = preprocessed_text\n",
    "\n",
    "# Pkl_Filename = \"X_train.pkl\"  \n",
    "\n",
    "# with open(Pkl_Filename, 'wb') as file:  \n",
    "#     pickle.dump(X_train, file)\n",
    "\n",
    "\n",
    "#preprocess data test\n",
    "preprocessed_text = []\n",
    "for i in range(0, len(test_X)):\n",
    "    preprocessed_text.append(process_tweet(test_X[i]))\n",
    "\n",
    "X_test = preprocessed_text\n",
    "\n",
    "# Pkl_Filename = \"y_train.pkl\"  \n",
    "\n",
    "# with open(Pkl_Filename, 'wb') as file:  \n",
    "#     pickle.dump(y_train, file)\n",
    "print(\"Selesai:\", datetime.now(time_jkt).strftime(\"%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa9978",
   "metadata": {},
   "source": [
    "# Create Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff9acae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionary(data):\n",
    "  dictionary = dict()\n",
    "  for sampel in  data:\n",
    "    for token in sampel:\n",
    "      dictionary[token] = dictionary.get(token, 0) + 1\n",
    "  #sorting dictionary berdasarkan nilainya\n",
    "  daftar_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "  return dict(daftar_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70460fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token teratas pada Dictionary:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('kuliah', 22299),\n",
       " ('online', 21185),\n",
       " ('ðŸ˜­', 3775),\n",
       " ('ga', 3229),\n",
       " ('yg', 2638),\n",
       " ('ya', 2529),\n",
       " ('aja', 2250),\n",
       " ('kalo', 2028),\n",
       " ('tugas', 1864),\n",
       " ('udah', 1841)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = createDictionary(X_train)\n",
    "\n",
    "print(\"Token teratas pada Dictionary:\\n\")\n",
    "list(bow.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af8bd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24665\n"
     ]
    }
   ],
   "source": [
    "print(len(bow))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda8d6eb",
   "metadata": {},
   "source": [
    "# NAIVE BAYES CLASIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ebb38c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Navie Bayes Classifier \n",
    "class NBClassifier:\n",
    "\n",
    "    def __init__(self, X_train, y_train, size):  \n",
    "      self.X_train = X_train\n",
    "      self.y_train = y_train\n",
    "      self.size = size\n",
    "\n",
    "    def createDictionary(self):\n",
    "      dictionary = dict()\n",
    "    \n",
    "      for sampel in  X_train:\n",
    "        for token in sampel:\n",
    "          dictionary[token] = dictionary.get(token, 0) + 1\n",
    "      daftar_dict = sorted(dictionary.items(), key=lambda x: x[1], reverse=True)\n",
    "      return dict(daftar_dict)\n",
    "    \n",
    "    def train(self):\n",
    "      X_train_dict = self.createDictionary()\n",
    "      if self.size == 'full':\n",
    "        self.daftar_kata = list(X_train_dict.keys())\n",
    "        self.jumlah_kata = dict.fromkeys(self.daftar_kata, None)\n",
    "        \n",
    "      else:\n",
    "        self.daftar_kata = list(X_train_dict.keys())[:int(self.size)]\n",
    "        self.jumlah_kata = dict.fromkeys(self.daftar_kata, None)\n",
    "      \n",
    "      train = pd.DataFrame(columns = ['X_train', 'y_train'])\n",
    "      train['X_train'] = X_train\n",
    "      train['y_train'] = y_train\n",
    "\n",
    "      train_0 = train.copy()[train['y_train'] == 0]\n",
    "      train_1 = train.copy()[train['y_train'] == 1]\n",
    "      train_2 = train.copy()[train['y_train'] == 2]\n",
    "\n",
    "      Prior_0 = train_0.shape[0]/train.shape[0]\n",
    "      Prior_1 = train_1.shape[0]/train.shape[0]\n",
    "      Prior_2 = train_2.shape[0]/train.shape[0]\n",
    "      \n",
    "      self.Prior = np.array([Prior_0, Prior_1, Prior_2])\n",
    "        \n",
    "      def flat(listOfList):\n",
    "        jadi = []\n",
    "        for elemen in listOfList:\n",
    "          jadi.extend(elemen)\n",
    "        return jadi\n",
    "  \n",
    "      X_train_0 = flat(train[train['y_train'] == 0]['X_train'].tolist())\n",
    "      X_train_1 = flat(train[train['y_train'] == 1]['X_train'].tolist())\n",
    "      X_train_2 = flat(train[train['y_train'] == 2]['X_train'].tolist())\n",
    "\n",
    "      self.X_train_len = np.array([len(X_train_0), len(X_train_1), len(X_train_2)])\n",
    "\n",
    "      for token in self.daftar_kata:\n",
    "        res = []\n",
    "        res.insert(0, X_train_0.count(token))\n",
    "        res.insert(1, X_train_1.count(token))\n",
    "        res.insert(2, X_train_2.count(token))\n",
    "        self.jumlah_kata[token] = res\n",
    "      return self\n",
    "\n",
    "    def predict(self, X_test):     \n",
    "      pred = []\n",
    "      \n",
    "      for sampel in X_test:\n",
    "            \n",
    "        mulai = np.array([1,1,1])\n",
    "        \n",
    "        for tokens in sampel:\n",
    "          jumlah_vocab = len(self.daftar_kata)\n",
    "          if tokens in self.daftar_kata:\n",
    "            prob = ((np.array(self.jumlah_kata[tokens])+1) / (self.X_train_len + jumlah_vocab))\n",
    "          else:\n",
    "            prob = ((np.array([0,0,0])+1) / (self.X_train_len + jumlah_vocab))\n",
    "          mulai = mulai * prob\n",
    "        print(mulai)\n",
    "        pos = mulai * self.Prior\n",
    "        pred.append(np.argmax(pos))\n",
    "      return pred\n",
    "    \n",
    "    def score(self, pred, labels):\n",
    "      correct = (np.array(pred) == np.array(labels)).sum()\n",
    "      accuracy = correct/len(pred)\n",
    "      return correct, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dab2a7bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.NBClassifier at 0x255fe8123a0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training classifier     \n",
    "nb = NBClassifier(X_train, y_train, 5)  \n",
    "nb.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "727923c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.33528991e-29 1.00430953e-32 1.03549641e-33]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "insert expected 2 arguments, got 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2284/2180753128.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2284/3425566707.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X_test)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmulai\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mpos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmulai\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPrior\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: insert expected 2 arguments, got 1"
     ]
    }
   ],
   "source": [
    "nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472af855",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
